{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d383bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea3ce82",
   "metadata": {},
   "source": [
    "### Carregando a Base e Definindo as amostras de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregarBase(base, isTodosAtributos, colunasRemovidas):\n",
    "    covidData = pd.read_excel(base)\n",
    "    if(isTodosAtributos == False):\n",
    "        covidData = covidData.drop(columns = colunasRemovidas)\n",
    "    return covidData\n",
    "\n",
    "def criarTreinamentoTeste(tamanhoTeste, baseDeDados, alvo):\n",
    "    return train_test_split(baseDeDados.drop(alvo, axis=1), baseDeDados[alvo], \n",
    "                            test_size = tamanhoTeste, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c88d4d",
   "metadata": {},
   "source": [
    "### Criando o modelo a partir do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarModelo(modelo, hiperparametros):\n",
    "    if(modelo == \"AB\"):\n",
    "        #classificador = AdaBoostClassifier(hiperparametros)\n",
    "        classificador = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
    "                                           n_estimators=50, random_state=410)\n",
    "        return classificador\n",
    "    \n",
    "    if(modelo == \"RF\"):\n",
    "        #classificador = RandomForestClassifier(hiperparametros)\n",
    "        classificador = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, \n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                       n_jobs=-1, oob_score=False, random_state=7374, verbose=0,\n",
    "                       warm_start=False)\n",
    "        return classificador\n",
    "    \n",
    "    if(modelo == \"LR\"):\n",
    "        #classificador = LogisticRegression(hiperparametros)\n",
    "        classificador = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=2576, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "        return classificador\n",
    "    \n",
    "    if(modelo == \"GB\"):\n",
    "        #classificador = GradientBoostingClassifier(hiperparametros)\n",
    "        classificador = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
    "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
    "                           max_features=None, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                           n_iter_no_change=None, \n",
    "                           random_state=2333, subsample=1.0, tol=0.0001,\n",
    "                           validation_fraction=0.1, verbose=0,\n",
    "                           warm_start=False)\n",
    "        return classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca36f84",
   "metadata": {},
   "source": [
    "### Calculando a acurácia, precisão, recall e f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularMetricas(x_train, x_test, y_train, y_test, baseDeDados, classificador, alvos):\n",
    "    metricas = {}\n",
    "    \n",
    "    # Fit nos dados\n",
    "    classificador.fit(x_train, y_train)\n",
    "\n",
    "    #Realizando a predição\n",
    "    resultadoPredicao = classificador.predict(x_test)\n",
    "\n",
    "    # Verificando a acurácia\n",
    "    metricas['Acurácia'] = __transformarEmPorcentagem__(accuracy_score(y_test, resultadoPredicao))\n",
    "    \n",
    "    # Criando um dicionaŕio a partir das métricas\n",
    "    metricasValores = metrics.classification_report(y_test, resultadoPredicao, output_dict=True)\n",
    "    df = pd.DataFrame(metricasValores).transpose()\n",
    "\n",
    "    for classe in alvos:\n",
    "        metricas[\"Precisão (\" + str(classe) + \")\"] = __transformarEmPorcentagem__(df.loc[str(classe)]['precision'])\n",
    "        metricas[\"Recall (\" + str(classe) + \")\"] = __transformarEmPorcentagem__(df.loc[str(classe)]['recall'])\n",
    "        metricas[\"F1-Score (\" + str(classe) + \")\"] = __transformarEmPorcentagem__(df.loc[str(classe)]['f1-score'])\n",
    "    \n",
    "    return metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43377f9b",
   "metadata": {},
   "source": [
    "### Calculando os K-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularKfolds(dados, alvo, classificador):\n",
    "    kfolds = {}\n",
    "    kfolds[\"3 Kfolds\"] = __transformarEmPorcentagem__(__calcularKfoldIndividual__(3, dados, alvo, classificador))\n",
    "    kfolds[\"5 Kfolds\"] = __transformarEmPorcentagem__(__calcularKfoldIndividual__(5, dados, alvo, classificador))\n",
    "    return kfolds\n",
    "    \n",
    "def __calcularKfoldIndividual__(kfolds, baseDeDados, alvo, classificador):\n",
    "    data, target = baseDeDados.drop(columns = [alvo]), baseDeDados[alvo]\n",
    "    scores = cross_val_score(classificador, data, target, cv = kfolds, scoring = \"accuracy\") \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e9ecd",
   "metadata": {},
   "source": [
    "### Utilitarios para transformação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9aab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __transformarEmPorcentagem__(valor):\n",
    "    porcentagem = str(round(valor * 100, 3)) + \"%\"\n",
    "    return porcentagem\n",
    "\n",
    "def converterMapas(baseMap, metricasMap, kfoldsMap):\n",
    "    baseMap.update(metricasMap)\n",
    "    baseMap.update(kfoldsMap)\n",
    "    return pd.DataFrame([baseMap])\n",
    "\n",
    "def juntarBases (base_1, base_2):\n",
    "    return pd.concat([base_1, base_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1fb662",
   "metadata": {},
   "source": [
    "### Avaliar os Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de65445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testarModelos(bases, alvo, colunasRemovidas, modelos, alvos):\n",
    "\n",
    "    desempenhoDosModelos = pd.DataFrame()\n",
    "    for nomeBase in bases.keys():\n",
    "        covidData = carregarBase(bases[nomeBase], False, colunasRemovidas)\n",
    "        x_train, x_test, y_train, y_test = criarTreinamentoTeste(0.3, covidData, alvo)\n",
    "\n",
    "        for nomeModelo in modelos.keys():\n",
    "            \n",
    "            #modelo = criarModelo(nomeModelo, modelos[nomeModelo])\n",
    "            modelo = modelos[nomeModelo]\n",
    "            baseMap = {'Base': nomeBase, 'Modelo': nomeModelo, 'Hiperparâmetros': modelo.get_params()}\n",
    "            \n",
    "            metricasMap = calcularMetricas(x_train, x_test, y_train, y_test, covidData, modelo, alvos)\n",
    "            kfoldsMap = calcularKfolds(covidData, alvo, modelo)\n",
    "            desempenhoDosModelos = juntarBases (desempenhoDosModelos, converterMapas(baseMap, metricasMap, kfoldsMap))\n",
    "\n",
    "    return desempenhoDosModelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66615bd7",
   "metadata": {},
   "source": [
    "### Salvando o dataframe em uma planilha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ba394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvarBaseDeDados(baseDeDados, arquivo):\n",
    "    write = pd.ExcelWriter(arquivo)\n",
    "    baseDeDados.to_excel(write, 'dados', index = False)\n",
    "    write.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce22cd",
   "metadata": {},
   "source": [
    "### Juntando os lotes da Primeira Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def juntarLotes(proporcao, reduzDados):\n",
    "    spLote1 = pd.read_excel(\"SP/sp-l1-\" + proporcao + \"-2022.xlsx\")\n",
    "    spLote2 = pd.read_excel(\"SP/sp-l2-\" + proporcao + \"-2022.xlsx\")\n",
    "    spLote3 = pd.read_excel(\"SP/sp-l3-\" + proporcao + \"-2022.xlsx\")\n",
    "    spLote4 = pd.read_excel(\"SP/sp-l4-\" + proporcao + \"-2022.xlsx\")\n",
    "    spLote5 = pd.read_excel(\"SP/sp-l5-\" + proporcao + \"-2022.xlsx\")\n",
    "\n",
    "    # Unindo os lotes\n",
    "    covidMerged = pd.concat([spLote1, spLote2, spLote3, spLote4, spLote5])\n",
    "\n",
    "    # Resetando os índices\n",
    "    covidMerged.reset_index(inplace=True, drop = True)\n",
    "    \n",
    "    if(reduzDados):\n",
    "        colunasRemovidas = [ 'disturbiosOlfativos', 'disturbiosGustatorios', 'puerpera', 'fragilidadeImuno', \n",
    "                    'gestante', 'obesidade']\n",
    "        covidMerged = covidMerged.drop(columns = colunasRemovidas)\n",
    "    \n",
    "    return covidMerged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1616ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
